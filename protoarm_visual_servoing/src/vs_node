#!/usr/bin/env python3

import sys

import cv2
import numpy as np
import rospy
import tf
from geometry_msgs.msg import Pose, PoseStamped
from sensor_msgs.msg import JointState

from tools.depth_image_processing import DepthImageProcessing

class VisualServoing:
    '''VisualServoing
    Implements position based visual servoing using depth image 
    '''

    def __init__(self):
        move_to_pose_topic = "/move_to_pose"
        move_to_joint_state_topic = "/move_to_joint_state"

        self.joint_names = ['Rev1', 'Rev2', 'Rev3', 'Rev4']

        self.pose_pub = rospy.Publisher(
            move_to_pose_topic, PoseStamped, queue_size=1000)
        self.joint_state_pub = rospy.Publisher(
            move_to_joint_state_topic, JointState, queue_size=1000)
        self.tf_listener = tf.TransformListener()

    def wait_for_kinematics(self):
        r = rospy.Rate(10)
        while(self.pose_pub.get_num_connections() == 0):
            print("Waiting for move_to_pose to get setup")
            r.sleep()

        while(self.joint_state_pub.get_num_connections() == 0):
            print("Waiting for move_to_joint_state to get setup")
            r.sleep()

        rospy.sleep(2)

    def move_to_scan_pose(self):

        SCAN_JOINT_STATE = [0, -0.25, 0.8, 1]
        joint_state = JointState()
        joint_state.position = SCAN_JOINT_STATE
        joint_state.name = self.joint_names

        self.joint_state_pub.publish(joint_state)

    def move_to_pose(self, pose):
        self.pose_pub.publish(pose)

    def _to_pose(self, joints, joint_names):
        x, y, z = joints
        pose = Pose()
        pose.position.x = float(x)
        pose.position.y = float(y)
        pose.position.z = float(z)
        return pose

    def focus_on_object(self, object_center, camera_center):
        if object_center and camera_center:
            object_center = np.array(object_center)
            camera_center = np.array(camera_center)
            diff = camera_center - camera_center

            try:
                (trans, rot) = self.tf_listener.lookupTransform(
                    '/camera_color_optical_frame', '/base_link', rospy.Time(0))
            except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
                pass

            left_finger_to_grip_pos = [-0.08636, 0, -0.0127]


if __name__ == '__main__':
    rospy.init_node('vs_node')

    vs = VisualServoing()
    depth_proc = DepthImageProcessing()

    vs.wait_for_kinematics()

    vs.move_to_scan_pose()

    vs.focus_on_object(depth_proc.object_center, depth_proc.camera_center)

    '''
        r = Rate(10)
        while not rospy.is_shutdown():
                target_coord = depth_proc.get_target_coord()    
                if(target_coord is not None):
                        print(target_coord)
                        #vs.move_to_pose(target_coord)  
                        break
                r.sleep()
        '''

    rospy.spin()
